{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters in the model\n",
    "Matrix R: containing the ratings \n",
    "Matrix U: Containing the embeddings of the users\n",
    "Matrix M: Containing the embedding of the movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from scipy.sparse import rand as sprand\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils import data\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grand dataset\n",
    "ratings_dataset = pd.read_csv('ratings_small.csv', usecols = ['userId','movieId','rating'])\n",
    "\n",
    "#ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new indices\n",
    "\n",
    "def proc_col(col, train_col=None):\n",
    "    \"\"\"Encodes a pandas column with continous ids. \"\"\"\n",
    "    if train_col is not None:\n",
    "        uniq = train_col.unique()\n",
    "    else:\n",
    "        uniq = col.unique()\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return name2idx, np.array([name2idx.get(x, -1) for x in col]), len(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(ratings_dataset, train=None):\n",
    "    \"\"\" Encodes rating data with continous user and movie ids. \n",
    "    If train is provided, encodes df with the same encoding as train.\n",
    "    \"\"\"\n",
    "    ratings_dataset = ratings_dataset.copy()\n",
    "    for col_name in [\"userId\", \"movieId\"]:\n",
    "        train_col = None\n",
    "        if train is not None:\n",
    "            train_col = train[col_name]\n",
    "        _,col,_ = proc_col(ratings_dataset[col_name], train_col)\n",
    "        ratings_dataset[col_name] = col\n",
    "        ratings_dataset = ratings_dataset[ratings_dataset[col_name] >= 0]\n",
    "    return ratings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dataset_e=encode_data(ratings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70002\n",
      "20001\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "# split train and validation before encoding\n",
    "trn_len = ratings_dataset_e.shape[0]- int(0.3*ratings_dataset_e.shape[0])\n",
    "val_len = int(0.2*ratings_dataset_e.shape[0])\n",
    "test_len = int(0.1*ratings_dataset_e.shape[0])\n",
    "\n",
    "train = ratings_dataset_e[:trn_len-1].copy()\n",
    "val = ratings_dataset_e[trn_len:-test_len].copy()\n",
    "test = ratings_dataset_e[trn_len:-val_len].copy()\n",
    "\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0]+ val.shape[0]+ test.shape[0] == ratings_dataset_e.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.max of         userId  movieId  rating\n",
       "0            0        0     2.5\n",
       "1            0        1     3.0\n",
       "2            0        2     3.0\n",
       "3            0        3     2.0\n",
       "4            0        4     4.0\n",
       "5            0        5     2.0\n",
       "6            0        6     2.0\n",
       "7            0        7     2.0\n",
       "8            0        8     3.5\n",
       "9            0        9     2.0\n",
       "10           0       10     2.5\n",
       "11           0       11     1.0\n",
       "12           0       12     4.0\n",
       "13           0       13     4.0\n",
       "14           0       14     3.0\n",
       "15           0       15     2.0\n",
       "16           0       16     2.0\n",
       "17           0       17     2.5\n",
       "18           0       18     1.0\n",
       "19           0       19     3.0\n",
       "20           1       20     4.0\n",
       "21           1       21     5.0\n",
       "22           1       22     5.0\n",
       "23           1       23     4.0\n",
       "24           1       24     4.0\n",
       "25           1       25     3.0\n",
       "26           1       26     3.0\n",
       "27           1       27     4.0\n",
       "28           1       28     3.0\n",
       "29           1       29     5.0\n",
       "...        ...      ...     ...\n",
       "99974      670      473     4.5\n",
       "99975      670      354     5.0\n",
       "99976      670      355     3.5\n",
       "99977      670     5577     4.0\n",
       "99978      670      477     5.0\n",
       "99979      670      478     5.0\n",
       "99980      670      358     4.5\n",
       "99981      670      479     4.5\n",
       "99982      670      480     5.0\n",
       "99983      670      359     4.0\n",
       "99984      670     1225     2.0\n",
       "99985      670     1240     2.0\n",
       "99986      670      361     3.0\n",
       "99987      670      126     4.0\n",
       "99988      670     1260     4.0\n",
       "99989      670      483     4.5\n",
       "99990      670      362     3.0\n",
       "99991      670      127     4.0\n",
       "99992      670      364     4.0\n",
       "99993      670     1299     3.5\n",
       "99994      670      412     5.0\n",
       "99995      670      486     4.0\n",
       "99996      670     1308     4.5\n",
       "99997      670      365     4.0\n",
       "99998      670     2930     2.5\n",
       "99999      670     7005     2.5\n",
       "100000     670     4771     4.0\n",
       "100001     670     1329     4.0\n",
       "100002     670     1331     2.5\n",
       "100003     670     2946     3.5\n",
       "\n",
       "[100004 rows x 3 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_dataset_e.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# Formatting the data \n",
    "\n",
    "class MovieLens(Dataset):\n",
    "  #'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self,users,movies,ratings):\n",
    "        'Initialization'\n",
    "        self.movies=movies\n",
    "        self.users=users\n",
    "        self.ratings=ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "     #   'Denotes the total number of samples'\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "     #   'Generates one sample of data'\n",
    "        # Select sample\n",
    "        \n",
    "        U = self.users[index]\n",
    "        V =self.movies[index]\n",
    "        y = self.ratings[index]\n",
    "        #obs = {'movieId':movieId,'userId':userId,'rating':rating}\n",
    "        #obs = self.movieLens.drop('timestamp',axis=1)\n",
    "        #obs = obs.iloc[index,:].as_matrix()\n",
    "\n",
    "        return [U,V,y]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = torch.LongTensor(train.userId.values)\n",
    "movies = torch.LongTensor(train.movieId.values)\n",
    "ratings = torch.FloatTensor(train.rating.values)\n",
    "\n",
    "train_dataset = MovieLens(users, movies, ratings)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid loader\n",
    "users_val = torch.LongTensor(val.userId.values)\n",
    "movies_val = torch.LongTensor(val.movieId.values)\n",
    "ratings_val = torch.FloatTensor(val.rating.values)\n",
    "\n",
    "val_dataset = MovieLens(users_val, movies_val, ratings_val)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# test loader\n",
    "users_test = torch.LongTensor(test.userId.values)\n",
    "movies_test = torch.LongTensor(test.movieId.values)\n",
    "ratings_test = torch.FloatTensor(test.rating.values)\n",
    "\n",
    "test_dataset = MovieLens(users_test, movies_test, ratings_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 287,   41,   59,   64,  293,  204,  471,  343,  198,   29,\n",
       "           22,  312,  210,  305,  305,  439,   89,  370,  383,  101,\n",
       "          261,  451,  231,  460,  307,  199,   14,  175,  101,   55,\n",
       "          427,  351,  125,  267,  298,  354,  460,  265,  284,  387,\n",
       "          471,   72,  191,   82,  415,  404,   48,   16,  184,  158,\n",
       "          305,  449,  345,  291,  467,  188,   14,  298,  452,  352,\n",
       "          387,  266,  305,  467,  231,  127,  412,  387,   95,   77,\n",
       "           91,  160,  291,   92,  235,  383,  467,  387,  479,  151,\n",
       "          469,  247,  344,  110,  380,   94,  212,  198,  478,  343,\n",
       "          293,  305,   18,  345,  293,  446,  137,   82,  456,   62]),\n",
       " tensor([   56,   106,  1289,  2990,  3385,  2397,   658,    78,   487,\n",
       "          2873,  2335,  4670,  5265,   714,   535,    71,   512,  1536,\n",
       "          1240,    79,  1107,  3098,    24,   151,    21,   131,   782,\n",
       "          1502,   243,  2309,  1038,   344,    70,   111,  6254,   202,\n",
       "           860,    64,  4088,   197,  1098,  1195,    92,   733,  2129,\n",
       "           353,  3340,  1972,  4590,   479,  2243,   332,  5009,  1412,\n",
       "          3762,   580,  1044,  1070,  1072,  3890,  2193,   143,  2952,\n",
       "           467,   197,  4644,    25,  2614,   439,    55,    20,   687,\n",
       "            59,   551,  1453,  1261,  7425,   861,    92,   179,  4089,\n",
       "           899,    99,  4081,  6749,  1103,  1500,  5124,  1654,    71,\n",
       "          5660,  2604,    80,  1369,  6146,   139,   575,   193,  1292,\n",
       "          1176]),\n",
       " tensor([ 3.0000,  4.0000,  4.0000,  5.0000,  3.0000,  4.5000,  5.0000,\n",
       "          5.0000,  3.5000,  4.0000,  5.0000,  3.0000,  4.0000,  3.0000,\n",
       "          4.0000,  4.0000,  5.0000,  5.0000,  3.5000,  4.0000,  0.5000,\n",
       "          3.0000,  5.0000,  1.5000,  4.0000,  1.5000,  5.0000,  4.0000,\n",
       "          4.0000,  4.0000,  3.5000,  4.0000,  3.0000,  2.5000,  4.0000,\n",
       "          5.0000,  0.5000,  4.0000,  2.0000,  4.0000,  4.0000,  3.0000,\n",
       "          3.0000,  4.5000,  1.0000,  3.5000,  3.0000,  3.5000,  3.0000,\n",
       "          4.5000,  5.0000,  5.0000,  4.5000,  4.0000,  2.5000,  1.0000,\n",
       "          3.0000,  4.5000,  3.0000,  3.5000,  5.0000,  4.5000,  3.0000,\n",
       "          2.5000,  5.0000,  2.0000,  3.0000,  3.0000,  3.5000,  4.5000,\n",
       "          2.0000,  3.0000,  4.0000,  4.0000,  3.5000,  1.5000,  3.0000,\n",
       "          4.0000,  4.5000,  4.0000,  4.5000,  4.0000,  5.0000,  3.5000,\n",
       "          2.0000,  4.0000,  2.5000,  3.0000,  4.5000,  4.0000,  3.5000,\n",
       "          2.0000,  3.0000,  5.0000,  4.0000,  3.0000,  3.0000,  5.0000,\n",
       "          3.0000,  4.0000])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing one batch from the train dataloader\n",
    "t1= iter(train_loader)\n",
    "next(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_user, num_movie, emb_size=100):\n",
    "        super(Model, self).__init__()\n",
    "        self.userEmb = nn.Embedding(num_user, emb_size)\n",
    "        self.movieEmb = nn.Embedding(num_movie, emb_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.userEmb(u)\n",
    "        V = self.movieEmb(v)\n",
    "        r_max=max(ratings_dataset.rating)\n",
    "        r_min=min(ratings_dataset.rating)\n",
    "        x = F.sigmoid((U*V).sum(1))*(r_max-r_min)+r_min\n",
    "        #x = (F.sigmoid((U*V).sum(1))*(r_max-r_min)+r_min)*0+2.7  #Test for randomness \n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = len(ratings_dataset.userId.unique()) \n",
    "num_movie = len(ratings_dataset.movieId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x11f0d38e0>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 100\n",
    "model=Model(num_user,num_movie,emb_size)\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 9.4142e-01, -4.2587e-01, -1.2396e+00,  ..., -6.9094e-01,\n",
       "          -2.2144e-01, -1.3675e+00],\n",
       "         [ 1.7064e+00, -3.9091e-01,  1.7707e+00,  ...,  1.1880e-01,\n",
       "          -1.3292e+00,  4.0134e-01],\n",
       "         [ 2.7273e-01,  5.2757e-01,  3.0223e-01,  ..., -1.4078e+00,\n",
       "          -8.8698e-01,  1.4852e+00],\n",
       "         ...,\n",
       "         [-3.3048e-01, -5.2376e-01,  1.5771e+00,  ...,  1.3146e+00,\n",
       "          -6.9357e-01, -7.4196e-01],\n",
       "         [-8.0809e-01, -2.6729e-01,  1.4588e+00,  ...,  5.1459e-01,\n",
       "          -3.8066e-01,  5.6025e-01],\n",
       "         [-9.0032e-02,  8.0352e-01,  5.9829e-01,  ...,  1.6233e+00,\n",
       "          -9.1059e-01, -2.1023e-01]]), Parameter containing:\n",
       " tensor([[-5.5235e-01,  1.1528e+00,  7.5984e-01,  ...,  5.6182e-01,\n",
       "          -1.4772e+00,  5.6303e-01],\n",
       "         [-6.0080e-01,  1.3087e+00,  4.1121e-02,  ..., -2.0679e+00,\n",
       "           9.1013e-01, -8.2162e-01],\n",
       "         [ 1.4930e+00,  2.2936e+00,  1.4063e+00,  ..., -3.8578e-01,\n",
       "          -1.0713e+00,  2.0182e+00],\n",
       "         ...,\n",
       "         [ 1.3585e+00,  9.4549e-01,  5.5129e-01,  ..., -9.4299e-03,\n",
       "          -1.7255e-01,  1.7113e+00],\n",
       "         [-8.5944e-02, -7.4222e-01, -7.7695e-01,  ..., -2.2550e-01,\n",
       "           8.9256e-01, -5.7724e-01],\n",
       "         [ 1.5759e+00, -3.4556e-01,  1.2396e+00,  ..., -7.1621e-01,\n",
       "          -2.9310e-01, -1.9027e+00]])]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(model):\n",
    "    model.eval() # go to evaluation mode\n",
    "    \n",
    "    running_loss = 0.\n",
    "    for j, data in enumerate(val_loader):\n",
    "        users, items, ratings = data\n",
    "        users = Variable(users)\n",
    "        items = Variable(items)\n",
    "        ratings = Variable(ratings).float()\n",
    "            \n",
    "       \n",
    "        y_hat = model(users,items)\n",
    "        loss_now = F.mse_loss(y_hat, ratings)\n",
    "        running_loss+= loss_now.item()\n",
    "        \n",
    "    print(\"validation loss\", \": \", running_loss/len(val_loader)) # j means this many iterations till end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    model.train() # into training mode\n",
    "    for i in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for j, data in enumerate(train_loader):\n",
    "            users, items, ratings = data\n",
    "            users = Variable(users)\n",
    "            items = Variable(items)\n",
    "            ratings = Variable(ratings).float()\n",
    "        \n",
    "\n",
    "            y_hat = model(users,items)\n",
    "            loss = F.mse_loss(y_hat, ratings)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(\"training loss for epoch \",i+1, \": \", running_loss/j+1) # used to be loss.data[0]\n",
    "    test_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  6.996513071911676\n",
      "training loss for epoch  2 :  6.0784609825270515\n",
      "training loss for epoch  3 :  5.567420956066677\n",
      "training loss for epoch  4 :  5.22586089606796\n",
      "training loss for epoch  5 :  4.904556610839707\n",
      "training loss for epoch  6 :  4.615669218472072\n",
      "training loss for epoch  7 :  4.339291962385177\n",
      "training loss for epoch  8 :  4.155947249276297\n",
      "training loss for epoch  9 :  4.013659203563417\n",
      "training loss for epoch  10 :  3.917520156928471\n",
      "validation loss :  6.164130286790838\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loss for epoch  1 :  tensor(79.6216)\n",
    "training loss for epoch  2 :  tensor(16.7696)\n",
    "validation loss :  tensor(98.1841)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  3.8320383638995033\n",
      "training loss for epoch  2 :  3.6992450504643575\n",
      "training loss for epoch  3 :  3.5054739369664873\n",
      "training loss for epoch  4 :  3.2740686551162175\n",
      "training loss for epoch  5 :  3.103051311118262\n",
      "validation loss :  1.9451256730959783\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss for epoch  1 :  1.1302297324527588\n",
      "training loss for epoch  2 :  1.1195393176509865\n",
      "training loss for epoch  3 :  1.1111403022387198\n",
      "training loss for epoch  4 :  1.1041365050551082\n",
      "training loss for epoch  5 :  1.0981343592330814\n",
      "validation loss :  14.090488851366944\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=5, lr=0.0001, wd=1e-6) # lower learning rate # lower regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering - Matrix factorization\n",
    "This script takes user and movie ratings as input to predict a user's rating of another movie, and thus recommend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Packages ####\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from scipy.sparse import rand as sprand\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils import data\n",
    "from torchtext.data import Dataset, BucketIterator, Field, TabularDataset, Iterator\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda\n"
     ]
    }
   ],
   "source": [
    "#### Activating cuda for speeding up training ####\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device in use:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading dataset ####\n",
    "# We are only interested in user ID, movie ID and ratings\n",
    "ratings_df = pd.read_csv('ratings_small.csv', usecols = ['userId','movieId','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing head of dataset with new IDs: \n",
      "   userId  movieId  rating\n",
      "0       0        0     2.5\n",
      "1       0        1     3.0\n",
      "2       0        2     3.0\n",
      "3       0        3     2.0\n",
      "4       0        4     4.0\n"
     ]
    }
   ],
   "source": [
    "#### Creating new indices ####\n",
    "def zero_indexing(column):\n",
    "    uniq = column.unique()\n",
    "    newindex = {o:i for i,o in enumerate(uniq)}\n",
    "    # Subtracting 1 from the original ID's\n",
    "    return newindex, np.array([newindex.get(x, -1) for x in column])\n",
    "\n",
    "_,usercol = zero_indexing(ratings_df[\"userId\"])\n",
    "_,moviecol = zero_indexing(ratings_df[\"movieId\"])\n",
    "\n",
    "ratings_df[\"userId\"] = usercol\n",
    "ratings_df[\"movieId\"] = moviecol\n",
    "\n",
    "# Removing ID's which are less than 0\n",
    "ratings_dataset = ratings_df[ratings_df[\"userId\"] >= 0]\n",
    "ratings_dataset = ratings_df[ratings_df[\"movieId\"] >= 0]\n",
    "\n",
    "print(\"Printing head of dataset with new IDs: \")\n",
    "print(ratings_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire dataset:  100004\n",
      "Train size:  70001\n",
      "Validation size:  20001\n",
      "Test size:  10002\n",
      "Checking if dimensions match:  True\n"
     ]
    }
   ],
   "source": [
    "#### Splitting dataset into training, validation and test sets ####\n",
    "n = len(ratings_dataset)\n",
    "sizes = [0.7, 0.2, 0.1]\n",
    "train_size = int(sizes[0]*n)\n",
    "val_size = int(sizes[1]*n)\n",
    "test_size = int(sizes[2]*n)\n",
    "\n",
    "train_set = ratings_dataset[:train_size-1].copy() # till 70002-1 = 70001\n",
    "val_set = ratings_dataset[train_size:-test_size-1].copy() # from 70002 to 100.004 - 20000\n",
    "test_set = ratings_dataset[train_size+val_size:].copy() # from train+val size\n",
    "\n",
    "print(\"Entire dataset: \", n)\n",
    "print(\"Train size: \", train_set.shape[0])\n",
    "print(\"Validation size: \", val_set.shape[0])\n",
    "print(\"Test size: \", test_set.shape[0])\n",
    "print(\"Checking if dimensions match: \", train_set.shape[0] + val_set.shape[0] + test_set.shape[0] == n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a dataformatting to format the data ####\n",
    "class dataformatting(Dataset):\n",
    "    def __init__(self, users, movies, rating):\n",
    "        self.movies = movies\n",
    "        self.users = users\n",
    "        self.rating = rating\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.rating)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        u = self.users[index]\n",
    "        m = self.movies[index]\n",
    "        r = self.rating[index]\n",
    "        #obs = {'movieId':movieId,'userId':userId,'rating':rating}\n",
    "        #obs = self.movieLens.drop('timestamp',axis=1)\n",
    "        #obs = obs.iloc[index,:].as_matrix()\n",
    "        return [u, m, r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Formatting training, validation and training sets ####\n",
    "u_train = torch.LongTensor(train_set.userId.values)\n",
    "m_train = torch.LongTensor(train_set.movieId.values)\n",
    "r_train = torch.FloatTensor(train_set.rating.values)\n",
    "\n",
    "train_dataset = dataformatting(u_train, m_train, r_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "u_val = torch.LongTensor(val_set.userId.values)\n",
    "m_val = torch.LongTensor(val_set.movieId.values)\n",
    "r_val = torch.FloatTensor(val_set.rating.values)\n",
    "val_dataset = dataformatting(u_val, m_val,r_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "u_test = torch.LongTensor(test_set.userId.values)\n",
    "m_test = torch.LongTensor(test_set.movieId.values)\n",
    "r_test = torch.FloatTensor(test_set.rating.values)\n",
    "test_dataset = dataformatting(u_test, m_test, r_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 653,  647,  606,  615,  647,  627,  597,  626,  663,  623,\n",
       "          623,  647,  597,  623,  614,  652,  653,  607,  623,  632,\n",
       "          614,  598,  607,  663,  623,  623,  610,  623,  623,  640,\n",
       "          627,  614,  666,  653,  604,  654,  604,  601,  663,  607,\n",
       "          626,  611,  623,  627,  670,  614,  613,  597,  622,  647,\n",
       "          636,  626,  607,  606,  608,  596,  622,  614,  657,  606,\n",
       "          623,  597,  636,  615,  670,  647,  624,  606,  606,  663,\n",
       "          663,  606,  653,  647,  654,  608,  663,  653,  645,  623,\n",
       "          604,  653,  651,  620,  638,  598,  604,  623,  640,  611,\n",
       "          608,  664,  659,  653,  613,  623,  614,  664,  623,  607]),\n",
       " tensor([ 3174,  7326,   151,  2073,  6230,   769,   481,  3901,  8684,\n",
       "          1706,  2363,  1244,   243,   968,   181,  9043,   386,   334,\n",
       "          2141,    81,   122,    29,  4212,  1509,   183,  4026,  4084,\n",
       "          5932,  8783,  3063,   416,  1619,   685,  1412,  4792,   487,\n",
       "           861,  1941,  5126,  2076,  1508,   332,   482,   479,   437,\n",
       "          1848,  1109,  5071,  1558,   348,   720,   281,   346,  1296,\n",
       "          6850,  7819,  2264,  1681,   459,  1409,  3179,   417,  2275,\n",
       "           102,   402,   342,    77,    61,   315,   569,   806,   430,\n",
       "           288,   201,  1365,  4865,  5169,   400,     1,    36,  4851,\n",
       "           331,  9014,  4497,  4134,   636,  4307,   170,  2520,  1041,\n",
       "          2727,  1941,  1631,    23,   432,  5108,  1092,     1,  3188,\n",
       "          1024]),\n",
       " tensor([ 4.0000,  3.5000,  2.0000,  4.0000,  3.5000,  2.5000,  4.0000,\n",
       "          4.0000,  3.5000,  3.5000,  3.0000,  4.5000,  4.0000,  4.0000,\n",
       "          4.0000,  3.0000,  4.0000,  3.0000,  3.0000,  3.0000,  4.5000,\n",
       "          1.5000,  4.0000,  4.0000,  4.0000,  2.5000,  1.0000,  3.5000,\n",
       "          3.0000,  3.0000,  4.0000,  4.0000,  4.0000,  4.5000,  2.0000,\n",
       "          3.0000,  1.0000,  3.0000,  3.5000,  3.0000,  4.0000,  4.0000,\n",
       "          3.0000,  3.0000,  3.0000,  4.5000,  4.0000,  2.0000,  4.0000,\n",
       "          3.0000,  5.0000,  1.5000,  4.0000,  4.0000,  5.0000,  5.0000,\n",
       "          2.0000,  3.0000,  4.0000,  4.0000,  3.0000,  4.5000,  3.5000,\n",
       "          4.0000,  4.5000,  2.0000,  3.5000,  3.5000,  0.5000,  3.5000,\n",
       "          3.5000,  1.0000,  4.5000,  3.0000,  4.0000,  1.0000,  2.0000,\n",
       "          4.0000,  4.0000,  2.0000,  2.0000,  4.5000,  4.0000,  3.5000,\n",
       "          3.0000,  5.0000,  2.0000,  5.0000,  3.0000,  3.0000,  1.0000,\n",
       "          2.0000,  5.0000,  4.5000,  1.0000,  2.0000,  4.0000,  4.0000,\n",
       "          1.5000,  4.0000])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = iter(train_loader)\n",
    "next(t1)\n",
    "t2 = iter(val_loader)\n",
    "next(t2)\n",
    "t3 = iter(test_loader)\n",
    "next(t3)\n",
    "#iter(train_batches.next())\n",
    "#iter(val_batches.next())\n",
    "#iter(test_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = len(ratings_dataset.userId.unique()) \n",
    "num_movie = len(ratings_dataset.movieId.unique())\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_user, num_movie, emb_size=100):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_user, emb_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movie, emb_size)\n",
    "        \n",
    "    def forward(self, u, m):\n",
    "        U = self.user_embedding(u)\n",
    "        M = self.movie_embedding(m)\n",
    "        r_max = 5 # maximum rating\n",
    "        r_min = 1 # minimum rating\n",
    "        return F.sigmoid((U*M).sum(1))*(r_max - r_min) + r_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.1312e+00, -1.7845e+00,  6.3465e-02,  ..., -5.0529e-02,\n",
       "           1.6703e-01,  8.5937e-01],\n",
       "         [-3.6144e-01, -1.3320e+00, -1.1073e-01,  ..., -3.0096e-01,\n",
       "           6.7772e-04, -7.5342e-01],\n",
       "         [-2.3945e+00,  5.5422e-01, -1.2960e+00,  ...,  1.1372e-01,\n",
       "           8.2862e-01,  1.7779e+00],\n",
       "         ...,\n",
       "         [ 9.7439e-02,  2.3393e-01, -1.8835e-03,  ...,  1.0384e+00,\n",
       "          -5.7337e-01,  2.3056e-01],\n",
       "         [ 2.9964e-01,  5.8796e-01, -7.5203e-01,  ..., -4.5147e-01,\n",
       "          -8.7082e-01,  8.8244e-01],\n",
       "         [ 1.5405e-01,  1.2580e+00,  1.3282e+00,  ...,  4.2246e-03,\n",
       "           9.2561e-01, -6.0783e-01]]), Parameter containing:\n",
       " tensor([[-0.9053, -0.3488,  0.8056,  ..., -0.4942, -0.2067,  0.0937],\n",
       "         [-0.2199, -0.9943,  1.5239,  ..., -0.5914,  0.7237, -0.3912],\n",
       "         [-0.3353,  0.7329,  2.1100,  ...,  1.9340,  0.3706, -0.3525],\n",
       "         ...,\n",
       "         [ 0.1909, -0.6452,  0.0886,  ...,  1.2922, -2.3757, -1.0737],\n",
       "         [-0.0180, -1.4869, -0.7618,  ...,  0.9452,  0.5447, -0.7084],\n",
       "         [-0.0147,  1.4134,  0.7827,  ...,  0.4836, -0.0953,  0.7407]])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MatrixFactorization(num_user, num_movie, 100)\n",
    "model.parameters()\n",
    "[p for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "wd = 1e-6\n",
    "\n",
    "def training_loss(model, epochs=10, lr=0.01, wd=0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    model.train() # into training mode\n",
    "    for i in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for j, data in enumerate(train_loader):\n",
    "            users, items, ratings = data\n",
    "            users = Variable(users)\n",
    "            items = Variable(items)\n",
    "            ratings = Variable(ratings).float()\n",
    "        \n",
    "            y_hat = model(users,items)\n",
    "            loss = F.mse_loss(y_hat, ratings) # crossentropy?\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(\"Training loss for epoch\",i+1,\":\", running_loss/j+1) # used to be loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(model):\n",
    "    model.eval() # go to evaluation mode\n",
    "    \n",
    "    running_loss = 0.\n",
    "    for j, data in enumerate(val_loader):\n",
    "        users, items, ratings = data\n",
    "        users = Variable(users)\n",
    "        items = Variable(items)\n",
    "        ratings = Variable(ratings).float()\n",
    "            \n",
    "        y_hat = model(users,items)\n",
    "        loss_now = F.mse_loss(y_hat, ratings)\n",
    "        running_loss+= loss_now.item()\n",
    "        \n",
    "    print(\"Validation loss:\", running_loss/len(val_batches)) # j means this many iterations till end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_loss(model, epochs=10, lr=0.01, wd = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training loop ####\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx in enumerate(train_loader):\n",
    "        users, items, ratings = data\n",
    "        users = Variable(users)\n",
    "        items = Variable(items)\n",
    "        ratings = Variable(ratings).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(users, items)\n",
    "        output = output.view(-1)\n",
    "\n",
    "        loss = criterion(output.float(), ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def validate(model, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    outputlist = []\n",
    "    val_loss = 0\n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in enumerate(val_loader):\n",
    "            users, items, ratings = data\n",
    "            users = Variable(users)\n",
    "            items = Variable(items)\n",
    "            ratings = Variable(ratings).float()\n",
    "            \n",
    "            output = model(users, items)\n",
    "            output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "            p = output_flat.count(1)\n",
    "            TP += p\n",
    "            FN += len(output_flat) - p\n",
    "            outputlist += [output]\n",
    "            val_loss += criterion(output.float(), ratings).item() # sum up batch loss\n",
    "\n",
    "    #print(TP, FN)\n",
    "    acc = (TP + TN)/(TP + TN + FP + FN)\n",
    "    #sens = TP/(TP + FN)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch}: Validation average loss: {val_loss:.2f} | Accuracy: {acc:.2f}')\n",
    "    return acc, val_loss\n",
    "\n",
    "def training(epochs, lr=0.001, wd = 1e-6): \n",
    "    model = MatrixFactorization(num_user, num_movie, 100)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay = wd)\n",
    "    \n",
    "    accs = []\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_loader, optimizer, criterion, epoch) # HER \n",
    "        acc, val_loss = validate(model, val_loader, criterion, epoch)# HER\n",
    "        accs += [acc]\n",
    "        losses += [val_loss]\n",
    "    \n",
    "    plt.plot(range(1,epochs+1), accs)\n",
    "    plt.show()\n",
    "    plt.plot(range(1,epochs+1), losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-714f187ab429>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-2e450f32b8bd>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(epochs, lr, wd)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# HER\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# HER\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0maccs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-2e450f32b8bd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0musers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not iterable"
     ]
    }
   ],
   "source": [
    "training(epochs=5, lr=0.0001, wd=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

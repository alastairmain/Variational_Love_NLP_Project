{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from scipy.sparse import rand as sprand\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils import data\n",
    "from torch.utils.data.dataset import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grand dataset\n",
    "ratings_dataset = pd.read_csv('ratings_small.csv', usecols = ['userId','movieId','rating'])\n",
    "#ratings\n",
    "#ratings.movieId\n",
    "ratings_dataset.rename(columns={'userId':'users',\n",
    "                          'movieId':'movies',\n",
    "                          'rating':'ratings'}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# Formatting the data \n",
    "\n",
    "class MovieLens(Dataset):\n",
    "  #'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self,movies,users,ratings):\n",
    "        'Initialization'\n",
    "        self.movies=movies\n",
    "        self.users=users\n",
    "        self.ratings=ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "     #   'Denotes the total number of samples'\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "     #   'Generates one sample of data'\n",
    "        # Select sample\n",
    "        \n",
    "        U = self.users[index]\n",
    "        V =self.movies[index]\n",
    "        y = self.ratings[index]\n",
    "        #obs = {'movieId':movieId,'userId':userId,'rating':rating}\n",
    "        #obs = self.movieLens.drop('timestamp',axis=1)\n",
    "        #obs = obs.iloc[index,:].as_matrix()\n",
    "\n",
    "        return [U,V,y]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "users = torch.LongTensor(ratings_dataset.users.values)\n",
    "movies = torch.LongTensor(ratings_dataset.movies.values)\n",
    "ratings = torch.FloatTensor(ratings_dataset.ratings.values)\n",
    "\n",
    "Movielens_dataset = MovieLens(users, movies, ratings)\n",
    "\n",
    "#x = MovieLens(csv_file='ratings_small.csv',root_dir=None)\n",
    "#print(x[1])\n",
    "#print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n",
      "100004\n"
     ]
    }
   ],
   "source": [
    "shuffle_dataset = True\n",
    "train_split=0.7\n",
    "validation_split=0.2\n",
    "test_split=0.1\n",
    "random_seed= 42\n",
    "dataset_size = len(Movielens_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "               \n",
    "split_1 = int(np.floor(train_split * dataset_size))\n",
    "split_2 = split_1+int(np.floor(validation_split * dataset_size))\n",
    "split_3 = split_2+int(np.floor(test_split * dataset_size))\n",
    "split_2end=split_3-split_2\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices, test_indices = indices[:split_1], indices[split_1+1:-split_2end], indices[split_2+1:]\n",
    "\n",
    "#train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(x, [train_size, test_size,val_size])\n",
    "\n",
    "train_dataset,val_dataset, test_dataset = random_split(Movielens_dataset, [len(train_indices), len(val_indices),len(test_indices)])\n",
    "\n",
    "#train_dataset,  val_dataset, test_dataset= Movielens_dataset[train_indices], Movielens_dataset[val_indices], Movielens_dataset[test_indices]\n",
    "\n",
    "print(len(train_indices) + len(val_indices)+len(test_indices))\n",
    "print(len(Movielens_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x11bba7b00>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([ 1.1930e+03,  5.8047e+04,  7.9430e+03,  7.8700e+02,  1.0305e+05,\n",
       "          5.4700e+02,  1.6800e+03,  9.1400e+02,  9.5000e+01,  9.2400e+02,\n",
       "          4.2800e+02,  2.9590e+03,  1.1000e+02,  9.9114e+04,  7.8200e+02,\n",
       "          3.6400e+02,  6.0800e+02,  2.1730e+03,  5.5410e+03,  1.1571e+05,\n",
       "          1.0970e+03,  1.0325e+05,  1.9090e+03,  1.7500e+02,  2.9490e+03,\n",
       "          1.3760e+03,  6.5370e+03,  1.2330e+03,  6.2970e+03,  4.5062e+04,\n",
       "          5.9000e+02,  2.9970e+03,  1.1000e+02,  2.7970e+03,  1.0360e+03,\n",
       "          1.6390e+03,  2.6820e+03,  2.6830e+03,  1.2400e+03,  8.7980e+03,\n",
       "          4.1280e+03,  1.2700e+03,  2.8900e+02,  2.0280e+03,  4.8890e+03,\n",
       "          1.3040e+03,  2.2710e+03,  1.2190e+03,  6.0510e+03,  8.8070e+03,\n",
       "          3.0000e+02,  8.8129e+04,  1.1970e+03,  3.3700e+02,  7.1057e+04,\n",
       "          1.5450e+03,  5.4100e+02,  2.9680e+03,  6.5390e+03,  4.2620e+03,\n",
       "          2.2520e+03,  4.7440e+03,  1.1310e+03,  9.4300e+02,  4.3200e+02,\n",
       "          2.6694e+04,  5.3080e+03,  4.9132e+04,  7.9132e+04,  5.8700e+02,\n",
       "          2.4680e+03,  9.5400e+02,  5.5820e+04,  4.8000e+02,  3.7510e+03,\n",
       "          3.0980e+03,  1.7220e+03,  2.4160e+03,  2.7450e+03,  4.0220e+03,\n",
       "          2.8040e+03,  4.0819e+04,  4.1500e+02,  1.9540e+03,  3.4400e+02,\n",
       "          1.4100e+02,  3.5780e+03,  2.8000e+02,  6.9790e+03,  2.0100e+03,\n",
       "          1.6300e+02,  1.5208e+05,  6.6100e+02,  7.4820e+03,  3.5240e+03,\n",
       "          3.0793e+04,  1.5370e+03,  5.0100e+02,  1.1990e+03,  1.6880e+03]),\n",
       " tensor([  15,   69,  587,  388,   73,  285,  342,  363,  562,  563,\n",
       "          596,  133,  628,  133,  128,  357,  120,  597,  624,  475,\n",
       "          130,  199,  407,  514,   15,   75,  471,  510,   68,  429,\n",
       "          527,  624,  522,  373,  563,  174,  564,  134,   73,   73,\n",
       "          367,  232,  239,    8,  509,  547,  564,  360,  648,  624,\n",
       "          511,  457,   83,  182,   48,  452,  137,  195,  309,  156,\n",
       "           30,  475,   94,  232,   85,  547,  139,  427,   73,  666,\n",
       "          232,  311,  501,  192,  294,  195,  435,  564,  509,  350,\n",
       "           46,  580,  514,   73,  262,   55,  324,  602,  460,  608,\n",
       "          422,  250,  213,  423,   58,  615,  467,  434,  514,  213]),\n",
       " tensor([ 5.0000,  3.0000,  4.5000,  5.0000,  4.0000,  3.0000,  5.0000,\n",
       "          5.0000,  3.5000,  5.0000,  4.5000,  3.0000,  1.5000,  3.5000,\n",
       "          3.0000,  5.0000,  1.5000,  3.0000,  4.0000,  5.0000,  3.0000,\n",
       "          4.0000,  4.0000,  5.0000,  4.0000,  0.5000,  3.5000,  5.0000,\n",
       "          3.5000,  1.0000,  3.5000,  4.0000,  4.0000,  2.0000,  4.0000,\n",
       "          4.0000,  5.0000,  5.0000,  4.5000,  3.0000,  5.0000,  5.0000,\n",
       "          5.0000,  4.0000,  4.0000,  5.0000,  5.0000,  4.0000,  4.0000,\n",
       "          3.0000,  5.0000,  4.0000,  5.0000,  2.0000,  3.5000,  4.0000,\n",
       "          4.0000,  3.0000,  4.0000,  4.5000,  4.0000,  1.5000,  4.5000,\n",
       "          5.0000,  4.0000,  4.0000,  4.5000,  4.0000,  4.0000,  2.0000,\n",
       "          4.0000,  2.5000,  4.0000,  4.0000,  4.0000,  3.0000,  3.0000,\n",
       "          3.0000,  4.0000,  3.0000,  5.0000,  3.5000,  1.0000,  4.5000,\n",
       "          2.5000,  4.0000,  3.5000,  3.0000,  5.0000,  3.0000,  4.0000,\n",
       "          5.0000,  1.5000,  4.0000,  4.0000,  2.0000,  4.0000,  1.0000,\n",
       "          4.0000,  3.0000])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size = 100,shuffle=True)\n",
    "\n",
    "print(train_dataloader)\n",
    "\n",
    "t1=iter(train_dataloader)\n",
    "next(t1)\n",
    "#for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "#    print(i_batch)\n",
    "#    print(sample_batched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1299,   593]), tensor([ 388,  527]), tensor([ 5.0000,  3.5000])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset,batch_size = 4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = DataLoader(val_dataset,batch_size = 4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 8010\n"
     ]
    }
   ],
   "source": [
    "# instantiating model class\n",
    "num_users = len(ratings_dataset.users[train_indices].unique())\n",
    "num_items = len(ratings_dataset.movies[train_indices].unique())\n",
    "print(num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_bias(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF_bias, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        # init \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        \n",
    "    def forward(self,userId,movieId):\n",
    "        U = self.user_emb(users)\n",
    "        V = self.item_emb(movies)\n",
    "        b_u = self.user_bias(users).squeeze()\n",
    "        b_v = self.item_bias(movies).squeeze()\n",
    "        return (U*V).sum(1) +  b_users  + b_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF_bias(num_users, num_items, emb_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    model.train() # into training mode\n",
    "    for i in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for j, data in enumerate(train_dataloader):\n",
    "            users, items, ratings = data\n",
    "            users = Variable(users)\n",
    "            items = Variable(items)\n",
    "            ratings = Variable(ratings).float()\n",
    "            \n",
    "            users = users.cuda() # put on gpu\n",
    "            items = items.cuda()\n",
    "            ratings = ratings.cuda()   \n",
    "            \n",
    "            if unsqueeze:\n",
    "                ratings = ratings.unsqueeze(1)\n",
    "            y_hat = model(users, items)\n",
    "            loss = F.mse_loss(y_hat, ratings)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.data[0]\n",
    "        print(\"training loss for epoch \",i+1, \": \", running_loss/j+1) # used to be loss.data[0]\n",
    "    test_loss(model, unsqueeze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70002])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([70002, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ratings = torch.FloatTensor(df_train.rating.values)\n",
    "ratings= torch.FloatTensor(ratings_dataset.ratings[train_indices].values)\n",
    "print(ratings.shape)\n",
    "ratings = ratings.unsqueeze(1)  #.cuda()\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 701, 2501)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataloader), len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defining test loss which has been used in train_epochs\n",
    "\n",
    "def test_loss(model, unsqueeze=False):\n",
    "    model.eval() # go to evaluation mode\n",
    "    \n",
    "    running_loss = 0.\n",
    "    for j, data in enumerate(valid_dataloader):\n",
    "        users, items, ratings = data\n",
    "        users, items, ratings = Variable(users), Variable(items), Variable(ratings)\n",
    "        \n",
    "        users = users.cuda() # put on gpu\n",
    "        items = items.cuda()\n",
    "        ratings = ratings.cuda()  \n",
    "        \n",
    "        if unsqueeze:\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "        y_hat = model(users, items)\n",
    "        loss_now = F.mse_loss(y_hat, ratings)\n",
    "        running_loss+= loss_now.data[0]\n",
    "    print(\"validation loss\", \": \", running_loss/len(valid_dataloader)) # j means this many iterations till end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cuda.LongTensor is not enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-48a22289249d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_epocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-18b8714e60f2>\u001b[0m in \u001b[0;36mtrain_epocs\u001b[0;34m(model, epochs, lr, wd, unsqueeze)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put on gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cuda.LongTensor is not enabled."
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEDESTÃ…ENDE ER GAMMEL LORT ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user=ratings.userId\n",
    "user = user.values\n",
    "\n",
    "item = ratings.movieId\n",
    "item = item.values\n",
    "\n",
    "\n",
    "#ratings = torch.tensor(ratings.values)\n",
    "#ratings=ratings.toarray()\n",
    "print(user)\n",
    "print(item)\n",
    "n_users = len(user)\n",
    "n_items= len(item)\n",
    "\n",
    "\n",
    "ratings = ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "mat = ratings.values\n",
    "mat[np.isnan(mat)] = 0\n",
    "#mat=mat.astype(int)\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users, \n",
    "                                               n_factors,\n",
    "                                               sparse=True)\n",
    "        #print(self.user_factors)\n",
    "        self.item_factors = torch.nn.Embedding(n_items, \n",
    "                                               n_factors,\n",
    "                                               sparse=True)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatrixFactorization(n_users, n_items, n_factors=20)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=1e-6) # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = mat.nonzero()\n",
    "print(rows)\n",
    "print(cols)\n",
    "p = np.random.permutation(len(rows))\n",
    "print(p)\n",
    "rows, cols = rows[p], cols[p]\n",
    "print(rows)\n",
    "print(cols)\n",
    "\n",
    "for row, col in zip(*(rows, cols)):\n",
    "    # Turn data into variables\n",
    "    rating = Variable(torch.FloatTensor([mat[row, col]]))\n",
    "    row = Variable(torch.LongTensor([np.long(row)]))\n",
    "    col = Variable(torch.LongTensor([np.long(col)]))\n",
    "    \n",
    "    # Predict and calculate loss\n",
    "    prediction = model(row, col)\n",
    "    loss = loss_func(prediction, rating)\n",
    "    \n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(rating)\n",
    "    print(row)\n",
    "    print(col)\n",
    "    print(prediction)\n",
    "    print(loss)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

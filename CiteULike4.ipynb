{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84?fbclid=IwAR1zjXlM5w93z47QalvvWeX7OQkIRGL8KK8dAkHepITnk81XFJt_g_FKdVE\n",
    "\n",
    "The code is heavily inspired from the above blogpost :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import csv\n",
    "from torchtext import data\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The User data \n",
    "User_data = pd.read_csv('data/user-info.csv', usecols = ['user.id','doc.id','rating'])\n",
    "User_data = User_data.rename(columns={'user.id': 'user_id','doc.id': 'doc_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The article dataset\n",
    "article_data = pd.read_csv('data/raw-data.csv', usecols = ['doc.id','title','citeulike.id', 'raw.abstract'],encoding = \"ISO-8859-1\")\n",
    "article_data = article_data.rename(columns={'raw.abstract': 'abstract','doc.id': 'doc_id','citeulike.id': 'citeulike_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CiteULike_data=pd.merge(User_data,article_data,on=\"doc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122938\n"
     ]
    }
   ],
   "source": [
    "le = max(len(x) for x in CiteULike_data.abstract)\n",
    "print(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = spacy.load('en')\n",
    "MAX_CHARS = 1229381\n",
    "VAL_RATIO = 0.2\n",
    "LOGGER = logging.getLogger(\"CiteULike_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(abs_text):\n",
    "    abs_text = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "        str(abs_text))\n",
    "    abs_text = re.sub(r\"[ ]+\", \" \", str(abs_text))\n",
    "    abs_text = re.sub(r\"\\!+\", \"!\", str(abs_text))\n",
    "    abs_text = re.sub(r\"\\,+\", \",\", str(abs_text))\n",
    "    abs_text = re.sub(r\"\\?+\", \"?\", str(abs_text))\n",
    "    if (len(abs_text) > MAX_CHARS):\n",
    "        abs_text = abs_text[:MAX_CHARS]\n",
    "    return [\n",
    "        x.text for x in NLP.tokenizer(abs_text) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_RATIO = 0.2\n",
    "\n",
    "def prepare_csv(seed=999):\n",
    "    df_train = CiteULike_data\n",
    "    df_train[\"abstract\"] = \\\n",
    "        df_train.abstract.str.replace(\"\\n\", \" \")\n",
    "    idx = np.arange(df_train.shape[0])\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idx)\n",
    "    val_size = int(len(idx) * VAL_RATIO)\n",
    "    df_train.iloc[idx[val_size:], :].to_csv(\n",
    "        \"cache/dataset_train.csv\", index=False)\n",
    "    df_train.iloc[idx[:val_size], :].to_csv(\n",
    "        \"cache/dataset_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(fix_length=100, lower=False, vectors=None):\n",
    "    if vectors is not None:\n",
    "        # pretrain vectors only supports all lower cases\n",
    "        lower = True\n",
    "    LOGGER.debug(\"Preparing CSV files...\")\n",
    "    prepare_csv()\n",
    "    abs_text = data.Field(\n",
    "        sequential=True,\n",
    "        fix_length=fix_length,\n",
    "        tokenize=tokenizer,\n",
    "        pad_first=True,\n",
    "        tensor_type=torch.LongTensor,\n",
    "        lower=lower\n",
    "    )\n",
    "    LOGGER.debug(\"Reading train csv file...\")\n",
    "    train, val = data.TabularDataset.splits(\n",
    "        path='cache/', format='csv', skip_header=True,\n",
    "        train='dataset_train.csv', validation='dataset_val.csv',\n",
    "        fields=[\n",
    "            ('abstract', abs_text),\n",
    "            ('doc_id', data.Field(\n",
    "                use_vocab=False, sequential=False, tensor_type=torch.ByteTensor)),\n",
    "            ('rating', data.Field(\n",
    "                use_vocab=False, sequential=False, tensor_type=torch.ByteTensor)),\n",
    "            ('title', data.Field(\n",
    "                use_vocab=False, sequential=False, tensor_type=torch.ByteTensor)),\n",
    "            ('citeulike_id', data.Field(\n",
    "                use_vocab=False, sequential=False, tensor_type=torch.ByteTensor)),\n",
    "            ('user_id', data.Field(\n",
    "                use_vocab=False, sequential=False, tensor_type=torch.ByteTensor)),\n",
    "        ])\n",
    "    LOGGER.debug(\"Building vocabulary...\")\n",
    "    abs_text.build_vocab(\n",
    "        train, val,\n",
    "        max_size=1229381,\n",
    "        min_freq=20,\n",
    "        vectors=vectors\n",
    "    )\n",
    "    LOGGER.debug(\"Done preparing the datasets\")\n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val=get_dataset(100,lower=False,vectors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator(dataset, batch_size, train=True, shuffle=True, repeat=False, sort=None,sort_key=None):\n",
    "    dataset_iter = data.Iterator(\n",
    "        dataset, batch_size=batch_size, device=-1,\n",
    "        train=train, shuffle=shuffle, repeat=repeat,\n",
    "        sort=sort,sort_key=sort_key\n",
    "    )\n",
    "    return dataset_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_iterator(\n",
    "            train, 100 ,train=True,\n",
    "            shuffle=True, repeat=False,sort_key=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Iterator' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-92de4e9f6b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Iterator' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get_iterator(\n",
    "            CiteULike_data, 100 ,train=True,\n",
    "            shuffle=True, repeat=False,sort_key=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         The Nucleic Acids Research Molecular Biology D...\n",
      "1         The Nucleic Acids Research Molecular Biology D...\n",
      "2         The Nucleic Acids Research Molecular Biology D...\n",
      "3         The Nucleic Acids Research Molecular Biology D...\n",
      "4         The Nucleic Acids Research Molecular Biology D...\n",
      "5         The Nucleic Acids Research Molecular Biology D...\n",
      "6         The Nucleic Acids Research Molecular Biology D...\n",
      "7         The Nucleic Acids Research Molecular Biology D...\n",
      "8         The Nucleic Acids Research Molecular Biology D...\n",
      "9         The Nucleic Acids Research Molecular Biology D...\n",
      "10        The Nucleic Acids Research Molecular Biology D...\n",
      "11        The Nucleic Acids Research Molecular Biology D...\n",
      "12        The Nucleic Acids Research Molecular Biology D...\n",
      "13        The Nucleic Acids Research Molecular Biology D...\n",
      "14        The Nucleic Acids Research Molecular Biology D...\n",
      "15        The Nucleic Acids Research Molecular Biology D...\n",
      "16        The Nucleic Acids Research Molecular Biology D...\n",
      "17        The Nucleic Acids Research Molecular Biology D...\n",
      "18        The Nucleic Acids Research Molecular Biology D...\n",
      "19        The Nucleic Acids Research Molecular Biology D...\n",
      "20        The Nucleic Acids Research Molecular Biology D...\n",
      "21        The Nucleic Acids Research Molecular Biology D...\n",
      "22        The Nucleic Acids Research Molecular Biology D...\n",
      "23        The Nucleic Acids Research Molecular Biology D...\n",
      "24        The Nucleic Acids Research Molecular Biology D...\n",
      "25        The Nucleic Acids Research Molecular Biology D...\n",
      "26        The Nucleic Acids Research Molecular Biology D...\n",
      "27        The Nucleic Acids Research Molecular Biology D...\n",
      "28        The Nucleic Acids Research Molecular Biology D...\n",
      "29        Cytoscape is an open source software project f...\n",
      "                                ...                        \n",
      "204956    Tracking and recognizing human face becomes on...\n",
      "204957    Indonesia has several laws and rules, one of t...\n",
      "204958    An intersection is a critical area of conflict...\n",
      "204959    Consolidation settlement is commonly computed ...\n",
      "204960    In this report we review the fundamentals, app...\n",
      "204961    1. Evidence-based policy requires researchers ...\n",
      "204962    1. Evidence-based policy requires researchers ...\n",
      "204963    Current spike sorting methods focus on cluster...\n",
      "204964    The purpose of this fourth Specialty Update is...\n",
      "204965    {Multivariate Calibration Harald Martens, Chem...\n",
      "204966    {Multivariate Calibration Harald Martens, Chem...\n",
      "204967    {Multivariate Calibration Harald Martens, Chem...\n",
      "204968    Although much research has examined the effect...\n",
      "204969    Although much research has examined the effect...\n",
      "204970    Although much research has examined the effect...\n",
      "204971    Microtubules (MTs) are important components of...\n",
      "204972    Microtubules (MTs) are important components of...\n",
      "204973    A low-complexity voice recognition method in a...\n",
      "204974    These third-year lecture notes are designed fo...\n",
      "204975    These third-year lecture notes are designed fo...\n",
      "204976    Although adaptive immunity is unique to verteb...\n",
      "204977    Although adaptive immunity is unique to verteb...\n",
      "204978    Para poder dar una definiciÃ³n de web 2.0 y ll...\n",
      "204979    Resumen: Se presenta el resultado de un estudi...\n",
      "204980    {Models are increasingly being relied upon to ...\n",
      "204981    Renewable energy has the potential to play an ...\n",
      "204982    After a long history of overexploitation, incr...\n",
      "204983    Virus-induced gene silencing (VIGS) is a techn...\n",
      "204984    Online chat rooms are a novel communication me...\n",
      "204985    Se pueden destacar tres cambios fundamentales ...\n",
      "Name: abstract, Length: 204986, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(g.dataset.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'ht tagging paper taxonomy flickr academic article to read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b3954521ab20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m for examples in get_iterator(\n\u001b[1;32m      2\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         ):\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m \u001b[0;31m# (fix_length, batch_size) Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 yield Batch(minibatch, self.dataset, self.device,\n\u001b[0;32m--> 151\u001b[0;31m                             self.train)\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device, train)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device, train)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \"\"\"\n\u001b[1;32m    187\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device, train)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 arr = [numericalization_func(x) if isinstance(x, six.string_types)\n\u001b[0;32m--> 306\u001b[0;31m                        else x for x in arr]\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 arr = [numericalization_func(x) if isinstance(x, six.string_types)\n\u001b[0;32m--> 306\u001b[0;31m                        else x for x in arr]\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'ht tagging paper taxonomy flickr academic article to read'"
     ]
    }
   ],
   "source": [
    "for examples in get_iterator(\n",
    "            train, 100 ,train=True,\n",
    "            shuffle=True, repeat=False,sort_key=True\n",
    "        ):\n",
    "    x = examples.abstract # (fix_length, batch_size) Tensor\n",
    "    y = torch.stack([\n",
    "        examples.doc_id, examples.rating, \n",
    "        examples.title,\n",
    "        examples.citeulike_id, examples.user_id\n",
    "    ], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204986"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters in the model\n",
    "Matrix R: containing the ratings \n",
    "Matrix U: Containing the embeddings of the users\n",
    "Matrix M: Containing the embedding of the movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Packages ####\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from scipy.sparse import rand as sprand\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils import data\n",
    "from torchtext.data import Dataset, BucketIterator, Field, TabularDataset, Iterator\n",
    "#from torchtext import data\n",
    "from torchtext import datasets\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn import Linear, RNN, LSTM\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# we'll use the bokeh library to create beautiful plots\n",
    "# *_notebook functions are needed for correct use in jupyter\n",
    "#from bokeh.plotting import figure, ColumnDataSource\n",
    "#from bokeh.models import HoverTool\n",
    "#from bokeh.io import output_notebook, show, push_notebook\n",
    "#output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grand dataset\n",
    "ratings_df = pd.read_csv('ratings_small.csv', usecols = ['userId','movieId','rating'])\n",
    "\n",
    "#ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing head of dataset with new IDs: \n",
      "   userId  movieId  rating\n",
      "0       0        0     2.5\n",
      "1       0        1     3.0\n",
      "2       0        2     3.0\n",
      "3       0        3     2.0\n",
      "4       0        4     4.0\n"
     ]
    }
   ],
   "source": [
    "#### Creating new indices ####\n",
    "def zero_indexing(column):\n",
    "    uniq = column.unique()\n",
    "    newindex = {o:i for i,o in enumerate(uniq)}\n",
    "    # Subtracting 1 from the original ID's\n",
    "    return newindex, np.array([newindex.get(x, -1) for x in column])\n",
    "\n",
    "_,usercol = zero_indexing(ratings_df[\"userId\"])\n",
    "_,moviecol = zero_indexing(ratings_df[\"movieId\"])\n",
    "\n",
    "ratings_df[\"userId\"] = usercol\n",
    "ratings_df[\"movieId\"] = moviecol\n",
    "\n",
    "# Removing ID's which are less than 0\n",
    "ratings_dataset = ratings_df[ratings_df[\"userId\"] >= 0]\n",
    "ratings_dataset = ratings_df[ratings_df[\"movieId\"] >= 0]\n",
    "\n",
    "print(\"Printing head of dataset with new IDs: \")\n",
    "print(ratings_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire dataset:  100004\n",
      "Train size:  70001\n",
      "Validation size:  20001\n",
      "Test size:  10002\n",
      "Checking if dimensions match:  True\n"
     ]
    }
   ],
   "source": [
    "#### Splitting dataset into training, validation and test sets ####\n",
    "n = len(ratings_dataset)\n",
    "sizes = [0.7, 0.2, 0.1]\n",
    "train_size = int(sizes[0]*n)\n",
    "val_size = int(sizes[1]*n)\n",
    "test_size = int(sizes[2]*n)\n",
    "\n",
    "train_set = ratings_dataset[:train_size-1].copy() # till 70002-1 = 70001\n",
    "val_set = ratings_dataset[train_size:-test_size-1].copy() # from 70002 to 100.004 - 20000\n",
    "test_set = ratings_dataset[train_size+val_size:].copy() # from train+val size\n",
    "\n",
    "print(\"Entire dataset: \", n)\n",
    "print(\"Train size: \", train_set.shape[0])\n",
    "print(\"Validation size: \", val_set.shape[0])\n",
    "print(\"Test size: \", test_set.shape[0])\n",
    "print(\"Checking if dimensions match: \", train_set.shape[0] + val_set.shape[0] + test_set.shape[0] == n )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a dataformatting to format the data ####\n",
    "class dataformatting(Dataset):\n",
    "    def __init__(self, users, movies, rating):\n",
    "        self.movies = movies\n",
    "        self.users = users\n",
    "        self.rating = rating\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.rating)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        u = self.users[index]\n",
    "        m = self.movies[index]\n",
    "        r = self.rating[index]\n",
    "        #obs = {'movieId':movieId,'userId':userId,'rating':rating}\n",
    "        #obs = self.movieLens.drop('timestamp',axis=1)\n",
    "        #obs = obs.iloc[index,:].as_matrix()\n",
    "        return [u, m, r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Formatting training, validation and training sets ####\n",
    "u_train = torch.LongTensor(train_set.userId.values)\n",
    "m_train = torch.LongTensor(train_set.movieId.values)\n",
    "r_train = torch.FloatTensor(train_set.rating.values)\n",
    "\n",
    "train_dataset = dataformatting(u_train, m_train, r_train)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "u_val = torch.LongTensor(val_set.userId.values)\n",
    "m_val = torch.LongTensor(val_set.movieId.values)\n",
    "r_val = torch.FloatTensor(val_set.rating.values)\n",
    "val_dataset = dataformatting(u_val, m_val,r_val)\n",
    "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "u_test = torch.LongTensor(test_set.userId.values)\n",
    "m_test = torch.LongTensor(test_set.movieId.values)\n",
    "r_test = torch.FloatTensor(test_set.rating.values)\n",
    "test_dataset = dataformatting(u_test, m_test, r_test)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing one batch from the train dataloader\n",
    "#t1= iter(train_iter)\n",
    "#next(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden=60\n",
    "num_user = len(ratings_dataset.userId.unique()) \n",
    "num_movie = len(ratings_dataset.movieId.unique())\n",
    "emb_size = 100\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, num_user, num_movie, emb_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.userEmb = nn.Embedding(num_user, emb_size)\n",
    "        self.movieEmb = nn.Embedding(num_movie, emb_size)\n",
    "        \n",
    "        self.lin = nn.Linear(in_features = emb_size*2, out_features= n_hidden, bias=True)\n",
    "        self.lin2 = nn.Linear(in_features = n_hidden, out_features=n_hidden,bias = True)\n",
    "        \n",
    "        self.lin3 = nn.Linear(in_features = n_hidden, out_features=1,bias = True)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(0.15)\n",
    "        self.drop2 = nn.Dropout(0.15)\n",
    "        self.drop3 = nn.Dropout(0.05)\n",
    "        # dropout??\n",
    "        # other stuff??\n",
    "        # ??????\n",
    "        \n",
    "        # Define the BatchNorm function (Converts the activations to have a variance of 1 and zero mean)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(n_hidden)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(n_hidden)\n",
    "        #self.bn3 = torch.nn.BatchNorm1d(1)\n",
    "       \n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        \n",
    "        U = self.userEmb(u)\n",
    "        V = self.movieEmb(v)\n",
    "        x = torch.cat([U, V], dim=1)\n",
    "      \n",
    "    \n",
    "        x = self.drop1(x)\n",
    "        x = self.lin(x)\n",
    "       \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        x = self.drop2(x)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.drop3(x)\n",
    "        x = self.lin3(x)\n",
    "       \n",
    "      \n",
    "        return torch.sigmoid(x)*4 + 1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing whether the model works\n",
    "#model=FFNN(num_user,num_movie,emb_size)\n",
    "#[p for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_iter, epoch):\n",
    "    #def validate(model, val_iter, criterion, epoch):\n",
    "    model.eval() # go to evaluation mode\n",
    "    \n",
    "    #Helps to measure accuracy\n",
    "    val_loss = 0\n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    \n",
    "    running_loss = 0.\n",
    "    for j, data in enumerate(val_iter):\n",
    "        users, items, ratings = data\n",
    "        users = Variable(users)\n",
    "        items = Variable(items)\n",
    "        ratings = Variable(ratings).float()\n",
    "        ratings=ratings.unsqueeze(1)\n",
    "            \n",
    "        y_hat = model(users, items)\n",
    "        loss_now = F.mse_loss(y_hat, ratings)\n",
    "        \n",
    "        # counts true positive, false negative etc. \n",
    "        for y,yhat in zip(ratings.data, y_hat):\n",
    "                y = int(y)\n",
    "                if yhat == 0:\n",
    "                    if y != yhat:\n",
    "                        FN += 1\n",
    "                    else:\n",
    "                        TN += 1\n",
    "                else:\n",
    "                    if y != yhat:\n",
    "                        FP += 1\n",
    "                    else:\n",
    "                        TP += 1\n",
    "        val_loss += loss_now.item() # sum up batch loss\n",
    "    \n",
    "    \n",
    "    val_acc = (TP + TN)/(TP + TN + FP + FN)\n",
    "    val_loss /= len(val_iter)\n",
    "    #print(f'Epoch {epoch:>3d} (100%) | Validation accucary: {acc:>2.5f}| Validation loss: {val_loss:>2.5f} \\n')\n",
    "    #print(\"validation loss\", \": \", val_loss/len(val_iter),\"validation acc\", \": \",acc ) # j means this many iterations till end\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, train_iter, optimizer, epoch,print_batch_p):\n",
    "    #def train_epocs(model, train_iter, optimizer, criterion, epoch):\n",
    "    \n",
    "    model.train() # into training mode\n",
    "    running_loss = 0.\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    loss_mean = []\n",
    "    acc_mean =[]\n",
    "           \n",
    "    for j, data in enumerate(train_iter):\n",
    "        users, items, ratings = data\n",
    "        batch_size = len(users)\n",
    "        users = Variable(users)\n",
    "        items = Variable(items)\n",
    "        ratings = Variable(ratings).float()\n",
    "        ratings= ratings.unsqueeze(1)\n",
    "        y_hat = model(users, items)\n",
    "        \n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print per percent\n",
    "        percent = print_batch_p\n",
    "        proc = (int((len(train_iter.dataset)/batch_size)*percent))\n",
    "        proc = proc if proc >= 1 else 1\n",
    "        \n",
    "        #calculate accuracy\n",
    "        TP,FP,TN,FN = 1,0,0,0\n",
    "        for y, yhat in zip(ratings, y_hat):\n",
    "            y = int(y)\n",
    "            if yhat == 0:\n",
    "                if y != yhat:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else:\n",
    "                if y != yhat:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "        acc_list += [(TP + TN)/(TP+FP+TN+FN)]\n",
    "        loss_list += [loss.item()]\n",
    "        \n",
    "   \n",
    "        loss_mean = sum(loss_list)/len(loss_list)\n",
    "        acc_mean = sum(acc_list)/len(acc_list)\n",
    "        #print(f'Epoch {epoch:3d} ({percent:3.0f}%) | Training accuracy: {acc_mean:2.5f} | Training loss: {loss_mean:2.5f}')\n",
    "        #print(\"training loss for epoch \",epoch+1, \": \", loss_mean,\"train acc\", \": \",acc_mean) # used to be loss.data[0]\n",
    "    return loss_mean, acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLoop(epochs, lr, wd,print_batch_p):\n",
    "    # Define model    \n",
    "    model = FFNN(num_user, num_movie,emb_size)\n",
    "    #criterion=F.mse_loss()\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    \n",
    "    accs = []\n",
    "    losses = []\n",
    "    best_acc = (-1.000005,-1e16,1e16)\n",
    "    best_loss = (-1.00005,-1e16,1e16)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_mean, acc_mean= train_epocs(model, train_iter, optimizer, epoch,print_batch_p )\n",
    "        val_acc, val_loss = validate(model, val_iter, epoch)\n",
    "        #train_epocs(model, train_iter, optimizer, criterion, epoch)\n",
    "        #acc, val_loss = validate(model, val_iter, criterion, epoch)\n",
    "        \n",
    "        #accs += [acc]\n",
    "        accs += [val_acc]\n",
    "        losses += [val_loss]\n",
    "            \n",
    "            \n",
    "    #print(f'Epoch {epoch:3d} ({percent:3.0f}%) | Training accuracy: {acc_mean:2.5f} | Training loss: {loss_mean:2.5f}')\n",
    "    #print(\"training loss for epoch \",epoch+1, \": \", loss_mean,\"train acc\", \": \",acc_mean) # used to be loss.data[0]\n",
    "    #print(f'Epoch {epoch:>3d} (100%) | Validation accucary: {acc:>2.5f}| Validation loss: {val_loss:>2.5f} \\n')\n",
    "    #print(\"validation loss\", \": \", val_loss,\"validation acc\", \": \",acc ) # j means this many iterations till end\n",
    "    print(\"Epoch\",epoch,\"|\", \"Training loss:\",loss_mean,\"|\", \"Validation loss:\",val_loss)\n",
    "    print(\"Epoch\",epoch,\"|\", \"Training accuracy:\",acc_mean,\"|\", \"Validation accuracy:\",val_acc)\n",
    "\n",
    "    #plt.plot(range(1,epoch+1),accs)\n",
    "    #plt.show()\n",
    "    #print(f'Best validation accuracy epoch: {best_acc[0]:>3.0f}, accuracy: {best_acc[1]:>2.5f}, mean loss: {best_acc[2]:>2.5f} \\n')\n",
    "    \n",
    "    #plt.plot(range(1,epoch+1),losses)\n",
    "    #plt.show()\n",
    "    #print(f'Best validation loss epoch: {best_loss[0]:>3.0f}, accuracy: {best_loss[1]:>2.5f}, mean loss: {best_loss[2]:>2.5f}')\n",
    "    \n",
    "    plt.figure()\n",
    "    epoch = np.arange(len(loss_list))\n",
    "    plt.plot(epoch, loss_list, 'r', label='Train Loss')\n",
    "    plt.plot(epoch, val_loss, 'b', label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Updates')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, acc_list, 'r', label='Train Acc')\n",
    "    plt.plot(epoch, val_acc, 'b', label='Val Acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Updates')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c60765895007>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_batch_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-a7de5b6e8bf8>\u001b[0m in \u001b[0;36mtrainLoop\u001b[1;34m(epochs, lr, wd, print_batch_p)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_mean\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_epocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_batch_p\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#train_epocs(model, train_iter, optimizer, criterion, epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-43c632b8e35d>\u001b[0m in \u001b[0;36mtrain_epocs\u001b[1;34m(model, train_iter, optimizer, epoch, print_batch_p)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Print per percent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainLoop(epochs=5, lr=0.025, wd = 1e-5,print_batch_p = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(Variable(torch.LongTensor(test.userId.values)),\n",
    "              Variable(torch.LongTensor(test.movieId.values)))\n",
    "\n",
    "y_hat.data.cpu().numpy().min(), y_hat.data.cpu().numpy().max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
